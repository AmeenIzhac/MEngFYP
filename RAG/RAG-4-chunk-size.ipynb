{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = \"INSERT_OPEN_API_KEY_HERE\"\n",
        "ANTHROPIC_API_KEY = \"INSERT_ANTHROPIC_API_KEY_HERE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(input_files=[\"anatomybook.pdf\"]).load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "nodes1 = SentenceSplitter(chunk_size=128, chunk_overlap=25).get_nodes_from_documents(documents)\n",
        "nodes2 = SentenceSplitter(chunk_size=256, chunk_overlap=50).get_nodes_from_documents(documents)\n",
        "nodes3 = SentenceSplitter(chunk_size=512, chunk_overlap=100).get_nodes_from_documents(documents)\n",
        "nodes4 = SentenceSplitter(chunk_size=1024, chunk_overlap=200).get_nodes_from_documents(documents)\n",
        "nodes5 = SentenceSplitter(chunk_size=2048, chunk_overlap=400).get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", api_key=OPENAI_API_KEY)\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\", api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "\n",
        "vector_index1 = VectorStoreIndex(nodes1)\n",
        "vector_index2 = VectorStoreIndex(nodes2)\n",
        "vector_index3 = VectorStoreIndex(nodes3)\n",
        "vector_index4 = VectorStoreIndex(nodes4)\n",
        "vector_index5 = VectorStoreIndex(nodes5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_engine1 = vector_index1.as_query_engine(chat_mode=\"best\", llm=OpenAI(model=\"gpt-3.5-turbo\"), verbose=True)\n",
        "query_engine2 = vector_index2.as_query_engine(chat_mode=\"best\", llm=OpenAI(model=\"gpt-3.5-turbo\"), verbose=True)\n",
        "query_engine3 = vector_index3.as_query_engine(chat_mode=\"best\", llm=OpenAI(model=\"gpt-3.5-turbo\"), verbose=True)\n",
        "query_engine4 = vector_index4.as_query_engine(chat_mode=\"best\", llm=OpenAI(model=\"gpt-3.5-turbo\"), verbose=True)\n",
        "query_engine5 = vector_index5.as_query_engine(chat_mode=\"best\", llm=OpenAI(model=\"gpt-3.5-turbo\"), verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_validation = load_dataset(\"openlifescienceai/medmcqa\", split=\"validation\").filter(lambda example: example[\"subject_name\"] == \"Anatomy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# query engine 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict1(prompt):\n",
        "    return str(query_engine1.query(prompt))\n",
        "\n",
        "def evaluate1():\n",
        "  SAMPLE_CNT = len(dataset_validation)\n",
        "  mismatch_cnt = 0\n",
        "  predictions = []\n",
        "  references = []\n",
        "\n",
        "  for i in range(SAMPLE_CNT):\n",
        "    example = dataset_validation[i]\n",
        "    question, option_a, option_b, option_c, option_d = example[\"question\"], example[\"opa\"], example[\"opb\"], example[\"opc\"], example[\"opd\"]\n",
        "    prompt = f'''{question}\n",
        "\n",
        "{option_a}\n",
        "{option_b}\n",
        "{option_c}\n",
        "{option_d}\n",
        "\n",
        "Respond with the correct choice from the list above verbatim.  Do not include any explanation.'''\n",
        "\n",
        "    options = [example['opa'], example['opb'], example['opc'], example['opd']]\n",
        "    correct_option = options[example['cop']]\n",
        "    references.append(correct_option)\n",
        "\n",
        "    prediction = predict1(prompt)\n",
        "    if prediction not in options:\n",
        "      prompt += prediction + \"\\n\\nYour response does not exactly match one of the choices from the list. Do not apologise or include any text other than one of the options from the list verbatim without any label. Here are the options again\\n\\n\" + example['opa'] + \"\\n\\n\" + example['opb'] + \"\\n\\n\" + example['opc'] + \"\\n\\n\" + example['opd']\n",
        "      prediction = predict1(prompt)\n",
        "\n",
        "    predictions.append(prediction)\n",
        "\n",
        "    mismatch_cnt += prediction not in options\n",
        "\n",
        "  exact_match = sum([prediction == reference for prediction, reference, in zip(predictions, references)]) / SAMPLE_CNT\n",
        "  mismatch = mismatch_cnt / SAMPLE_CNT\n",
        "\n",
        "  return exact_match, mismatch\n",
        "\n",
        "exact_match, mismatch = evaluate1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "exact_match: 0.5854700854700855\n",
            "mismatch: 0.021367521367521368\n"
          ]
        }
      ],
      "source": [
        "print(\"exact_match:\", exact_match)\n",
        "print(\"mismatch:\", mismatch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# query engine 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict2(prompt):\n",
        "    return str(query_engine2.query(prompt))\n",
        "\n",
        "def evaluate2():\n",
        "  SAMPLE_CNT = len(dataset_validation)\n",
        "  mismatch_cnt = 0\n",
        "  predictions = []\n",
        "  references = []\n",
        "\n",
        "  for i in range(SAMPLE_CNT):\n",
        "    example = dataset_validation[i]\n",
        "    question, option_a, option_b, option_c, option_d = example[\"question\"], example[\"opa\"], example[\"opb\"], example[\"opc\"], example[\"opd\"]\n",
        "    prompt = f'''{question}\n",
        "\n",
        "{option_a}\n",
        "{option_b}\n",
        "{option_c}\n",
        "{option_d}\n",
        "\n",
        "Respond with the correct choice from the list above verbatim.  Do not include any explanation.'''\n",
        "\n",
        "    options = [example['opa'], example['opb'], example['opc'], example['opd']]\n",
        "    correct_option = options[example['cop']]\n",
        "    references.append(correct_option)\n",
        "\n",
        "    prediction = predict2(prompt)\n",
        "    if prediction not in options:\n",
        "      prompt += prediction + \"\\n\\nYour response does not exactly match one of the choices from the list. Do not apologise or include any text other than one of the options from the list verbatim without any label. Here are the options again\\n\\n\" + example['opa'] + \"\\n\\n\" + example['opb'] + \"\\n\\n\" + example['opc'] + \"\\n\\n\" + example['opd']\n",
        "      prediction = predict2(prompt)\n",
        "\n",
        "    predictions.append(prediction)\n",
        "\n",
        "    mismatch_cnt += prediction not in options\n",
        "\n",
        "  exact_match = sum([prediction == reference for prediction, reference, in zip(predictions, references)]) / SAMPLE_CNT\n",
        "  mismatch = mismatch_cnt / SAMPLE_CNT\n",
        "\n",
        "  return exact_match, mismatch\n",
        "\n",
        "exact_match, mismatch = evaluate2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "exact_match: 0.5769230769230769\n",
            "mismatch: 0.017094017094017096\n"
          ]
        }
      ],
      "source": [
        "print(\"exact_match:\", exact_match)\n",
        "print(\"mismatch:\", mismatch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# query engine 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict3(prompt):\n",
        "    return str(query_engine3.query(prompt))\n",
        "\n",
        "def evaluate3():\n",
        "  SAMPLE_CNT = len(dataset_validation)\n",
        "  mismatch_cnt = 0\n",
        "  predictions = []\n",
        "  references = []\n",
        "\n",
        "  for i in range(SAMPLE_CNT):\n",
        "    example = dataset_validation[i]\n",
        "    question, option_a, option_b, option_c, option_d = example[\"question\"], example[\"opa\"], example[\"opb\"], example[\"opc\"], example[\"opd\"]\n",
        "    prompt = f'''{question}\n",
        "\n",
        "{option_a}\n",
        "{option_b}\n",
        "{option_c}\n",
        "{option_d}\n",
        "\n",
        "Respond with the correct choice from the list above verbatim.  Do not include any explanation.'''\n",
        "\n",
        "    options = [example['opa'], example['opb'], example['opc'], example['opd']]\n",
        "    correct_option = options[example['cop']]\n",
        "    references.append(correct_option)\n",
        "\n",
        "    prediction = predict3(prompt)\n",
        "    if prediction not in options:\n",
        "      prompt += prediction + \"\\n\\nYour response does not exactly match one of the choices from the list. Do not apologise or include any text other than one of the options from the list verbatim without any label. Here are the options again\\n\\n\" + example['opa'] + \"\\n\\n\" + example['opb'] + \"\\n\\n\" + example['opc'] + \"\\n\\n\" + example['opd']\n",
        "      prediction = predict3(prompt)\n",
        "\n",
        "    predictions.append(prediction)\n",
        "\n",
        "    mismatch_cnt += prediction not in options\n",
        "\n",
        "  exact_match = sum([prediction == reference for prediction, reference, in zip(predictions, references)]) / SAMPLE_CNT\n",
        "  mismatch = mismatch_cnt / SAMPLE_CNT\n",
        "\n",
        "  return exact_match, mismatch\n",
        "\n",
        "exact_match, mismatch = evaluate3()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "exact_match: 0.5683760683760684\n",
            "mismatch: 0.03418803418803419\n"
          ]
        }
      ],
      "source": [
        "print(\"exact_match:\", exact_match)\n",
        "print(\"mismatch:\", mismatch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# query engine 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict4(prompt):\n",
        "    return str(query_engine4.query(prompt))\n",
        "\n",
        "def evaluate4():\n",
        "  SAMPLE_CNT = len(dataset_validation)\n",
        "  mismatch_cnt = 0\n",
        "  predictions = []\n",
        "  references = []\n",
        "\n",
        "  for i in range(SAMPLE_CNT):\n",
        "    example = dataset_validation[i]\n",
        "    question, option_a, option_b, option_c, option_d = example[\"question\"], example[\"opa\"], example[\"opb\"], example[\"opc\"], example[\"opd\"]\n",
        "    prompt = f'''{question}\n",
        "\n",
        "{option_a}\n",
        "{option_b}\n",
        "{option_c}\n",
        "{option_d}\n",
        "\n",
        "Respond with the correct choice from the list above verbatim.  Do not include any explanation.'''\n",
        "\n",
        "    options = [example['opa'], example['opb'], example['opc'], example['opd']]\n",
        "    correct_option = options[example['cop']]\n",
        "    references.append(correct_option)\n",
        "\n",
        "    prediction = predict4(prompt)\n",
        "    if prediction not in options:\n",
        "      prompt += prediction + \"\\n\\nYour response does not exactly match one of the choices from the list. Do not apologise or include any text other than one of the options from the list verbatim without any label. Here are the options again\\n\\n\" + example['opa'] + \"\\n\\n\" + example['opb'] + \"\\n\\n\" + example['opc'] + \"\\n\\n\" + example['opd']\n",
        "      prediction = predict4(prompt)\n",
        "\n",
        "    predictions.append(prediction)\n",
        "\n",
        "    mismatch_cnt += prediction not in options\n",
        "\n",
        "  exact_match = sum([prediction == reference for prediction, reference, in zip(predictions, references)]) / SAMPLE_CNT\n",
        "  mismatch = mismatch_cnt / SAMPLE_CNT\n",
        "\n",
        "  return exact_match, mismatch\n",
        "\n",
        "exact_match, mismatch = evaluate4()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "exact_match: 0.5897435897435898\n",
            "mismatch: 0.02564102564102564\n"
          ]
        }
      ],
      "source": [
        "print(\"exact_match:\", exact_match)\n",
        "print(\"mismatch:\", mismatch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# query engine 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict5(prompt):\n",
        "    return str(query_engine5.query(prompt))\n",
        "\n",
        "def evaluate5():\n",
        "  SAMPLE_CNT = len(dataset_validation)\n",
        "  mismatch_cnt = 0\n",
        "  predictions = []\n",
        "  references = []\n",
        "\n",
        "  for i in range(SAMPLE_CNT):\n",
        "    example = dataset_validation[i]\n",
        "    question, option_a, option_b, option_c, option_d = example[\"question\"], example[\"opa\"], example[\"opb\"], example[\"opc\"], example[\"opd\"]\n",
        "    prompt = f'''{question}\n",
        "\n",
        "{option_a}\n",
        "{option_b}\n",
        "{option_c}\n",
        "{option_d}\n",
        "\n",
        "Respond with the correct choice from the list above verbatim.  Do not include any explanation.'''\n",
        "\n",
        "    options = [example['opa'], example['opb'], example['opc'], example['opd']]\n",
        "    correct_option = options[example['cop']]\n",
        "    references.append(correct_option)\n",
        "\n",
        "    prediction = predict5(prompt)\n",
        "    if prediction not in options:\n",
        "      prompt += prediction + \"\\n\\nYour response does not exactly match one of the choices from the list. Do not apologise or include any text other than one of the options from the list verbatim without any label. Here are the options again\\n\\n\" + example['opa'] + \"\\n\\n\" + example['opb'] + \"\\n\\n\" + example['opc'] + \"\\n\\n\" + example['opd']\n",
        "      prediction = predict5(prompt)\n",
        "\n",
        "    predictions.append(prediction)\n",
        "\n",
        "    mismatch_cnt += prediction not in options\n",
        "\n",
        "  exact_match = sum([prediction == reference for prediction, reference, in zip(predictions, references)]) / SAMPLE_CNT\n",
        "  mismatch = mismatch_cnt / SAMPLE_CNT\n",
        "\n",
        "  return exact_match, mismatch\n",
        "\n",
        "exact_match, mismatch = evaluate5()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "exact_match: 0.5982905982905983\n",
            "mismatch: 0.029914529914529916\n"
          ]
        }
      ],
      "source": [
        "print(\"exact_match:\", exact_match)\n",
        "print(\"mismatch:\", mismatch)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
