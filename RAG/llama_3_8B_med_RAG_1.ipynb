{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwuW84ehxkT5"
      },
      "source": [
        "# imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsboHtg6INBb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install \"xformers<0.0.26\" trl peft accelerate bitsandbytes pinecone-client sentence-transformers langchain-openai openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJExmmYaAdee"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TrH4FaQ_Sel"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = torch.float16,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 256,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 512,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOTDVBOMEQYQ"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key='INSERT_PINECONE_API_KEY_HERE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8SBuaNCFxH1"
      },
      "outputs": [],
      "source": [
        "index = pc.Index(\"medmcqa-train-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys16bnn7JdEX"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=\"INSERT_OPEN_API_KEY_HERE\")\n",
        "\n",
        "def get_relevant_context(text, k=1):\n",
        "  embeds = embed_model.embed_documents([text])\n",
        "  results_1 = index.query(\n",
        "      vector=embeds[0],\n",
        "      top_k=k,\n",
        "      include_metadata=True\n",
        "  )\n",
        "\n",
        "  results = []\n",
        "\n",
        "  for result in results_1[\"matches\"]:\n",
        "    results.append(result[\"metadata\"][\"text\"])\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MNB8HXQxfYw"
      },
      "source": [
        "# load datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbZadzvn-Hb7"
      },
      "outputs": [],
      "source": [
        "medmcqa = load_dataset(\"openlifescienceai/medmcqa\", split=\"validation\")\n",
        "medmcqa_mixed = medmcqa.select(range(234))\n",
        "medmcqa_anatomy = medmcqa.filter(lambda example: example[\"subject_name\"] == \"Anatomy\")\n",
        "\n",
        "medqa = load_dataset(\"GBaker/MedQA-USMLE-4-options\", split=\"test\").select(range(234))\n",
        "\n",
        "pubmedqa = load_dataset(\"bigbio/pubmed_qa\", split=\"validation\").select(range(234))\n",
        "\n",
        "mmlu_anatomy = load_dataset(\"openlifescienceai/mmlu_anatomy\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOcUQyodCGWH"
      },
      "outputs": [],
      "source": [
        "print(medmcqa_mixed)\n",
        "print(medmcqa_anatomy)\n",
        "print(medqa)\n",
        "print(pubmedqa)\n",
        "print(mmlu_anatomy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFQGgY3HCZ63"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o74N2rVoCkRL"
      },
      "outputs": [],
      "source": [
        "def predict(prompt):\n",
        "    inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "    return tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens = True)[0].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S35ZtEvoMxL"
      },
      "source": [
        "# medmcqa mixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4s7UWpoG66h"
      },
      "outputs": [],
      "source": [
        "def eval_medmcqa_mixed():\n",
        "  references = []\n",
        "  predictions = []\n",
        "  SAMPLE_CNT = len(medmcqa_mixed)\n",
        "  mismatch_cnt = 0\n",
        "\n",
        "  for i in range(SAMPLE_CNT):\n",
        "    example = medmcqa_mixed[i]\n",
        "    options = [example['opa'], example['opb'], example['opc'], example['opd']]\n",
        "\n",
        "    prompt = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\" + example[\"question\"] + \"\\n\\n\" + example['opa'] + \"\\n\" + example['opb'] + \"\\n\" + example['opc'] + \"\\n\" + example['opd'] + \"\\n\\nRespond with the correct choice from the list above verbatim. Do not include any explanation.\"\n",
        "\n",
        "    context = get_relevant_context(example[\"question\"] + \"\\n\\n\" + example['opa'] + \"\\n\" + example['opb'] + \"\\n\" + example['opc'] + \"\\n\" + example['opd'])\n",
        "    prompt += \" You may use the following information only if it is helpful: \\n\" + context[0]\n",
        "\n",
        "    prompt += \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    prediction = predict(prompt)\n",
        "\n",
        "    if prediction not in options:\n",
        "      prompt += prediction + \"<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\nYour response does not exactly match one of the choices from the list. Do not apologise or include any text other than one of the options from the list verbatim without any label. Here are the options again\\n\\n\" + example['opa'] + \"\\n\\n\" + example['opb'] + \"\\n\\n\" + example['opc'] + \"\\n\\n\" + example['opd'] + \"\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "      prediction = predict(prompt)\n",
        "\n",
        "    reference = options[example['cop']]\n",
        "\n",
        "    if prediction not in options:\n",
        "      mismatch_cnt += 1\n",
        "\n",
        "    references.append(reference)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "  em = sum([1 if prediction == reference else 0 for prediction, reference in zip(predictions, references)]) / SAMPLE_CNT\n",
        "  mismatch = mismatch_cnt / SAMPLE_CNT\n",
        "\n",
        "  return em, mismatch\n",
        "\n",
        "em, mismatch = eval_medmcqa_mixed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUX05IpDH00Y",
        "outputId": "02a381d1-5a46-465a-d513-2162d7039cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exact match: 0.5854700854700855\n",
            "mismatch: 0.01282051282051282\n"
          ]
        }
      ],
      "source": [
        "print(\"exact match:\", em)\n",
        "print(\"mismatch:\", mismatch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHQLbWfqymDx"
      },
      "source": [
        "# medmcqa anatomy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzPfBRu-rq33"
      },
      "outputs": [],
      "source": [
        "def eval_medmcqa_anatomy():\n",
        "  references = []\n",
        "  predictions = []\n",
        "  SAMPLE_CNT = len(medmcqa_anatomy)\n",
        "  mismatch_cnt = 0\n",
        "\n",
        "  for i in range(SAMPLE_CNT):\n",
        "    example = medmcqa_anatomy[i]\n",
        "    options = [example['opa'], example['opb'], example['opc'], example['opd']]\n",
        "\n",
        "    prompt = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\" + example[\"question\"] + \"\\n\\n\" + example['opa'] + \"\\n\" + example['opb'] + \"\\n\" + example['opc'] + \"\\n\" + example['opd'] + \"\\n\\nRespond with the correct choice from the list above verbatim. Do not include any explanation.\"\n",
        "\n",
        "    context = get_relevant_context(example[\"question\"] + \"\\n\\n\" + example['opa'] + \"\\n\" + example['opb'] + \"\\n\" + example['opc'] + \"\\n\" + example['opd'])\n",
        "    prompt += \" You may use the following information only if it is helpful: \\n\" + context[0]\n",
        "\n",
        "    prompt += \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    prediction = predict(prompt)\n",
        "\n",
        "    if prediction not in options:\n",
        "      prompt += prediction + \"<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\nYour response does not exactly match one of the choices from the list. Do not apologise or include any text other than one of the options from the list verbatim without any label. Here are the options again\\n\\n\" + example['opa'] + \"\\n\\n\" + example['opb'] + \"\\n\\n\" + example['opc'] + \"\\n\\n\" + example['opd'] + \"\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "      prediction = predict(prompt)\n",
        "\n",
        "    reference = options[example['cop']]\n",
        "\n",
        "    if prediction not in options:\n",
        "      mismatch_cnt += 1\n",
        "\n",
        "    references.append(reference)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "  em = sum([1 if prediction == reference else 0 for prediction, reference in zip(predictions, references)]) / SAMPLE_CNT\n",
        "  mismatch = mismatch_cnt / SAMPLE_CNT\n",
        "\n",
        "  return em, mismatch\n",
        "\n",
        "em, mismatch = eval_medmcqa_anatomy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeI1YqnQu75U",
        "outputId": "2ef8b5e4-9862-4dbf-89c5-f73b2dd75b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exact match: 0.6367521367521367\n",
            "mismatch: 0.021367521367521368\n"
          ]
        }
      ],
      "source": [
        "print(\"exact match:\", em)\n",
        "print(\"mismatch:\", mismatch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZJ220l6yqnI"
      },
      "source": [
        "# medqa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhVvpRsUBC9W"
      },
      "outputs": [],
      "source": [
        "def eval_medqa():\n",
        "  references = []\n",
        "  predictions = []\n",
        "  SAMPLE_CNT = len(medqa)\n",
        "  mismatch_cnt = 0\n",
        "  both_right = 0\n",
        "  both_wrong = 0\n",
        "  only_rag = 0\n",
        "  only_without = 0\n",
        "\n",
        "  for i in range(SAMPLE_CNT):\n",
        "    example = medqa[i]\n",
        "    options = [example['options']['A'], example['options']['B'], example['options']['C'], example['options']['D']]\n",
        "\n",
        "    prompt = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\" + example[\"question\"] + \"\\n\\n\" + example['options']['A'] + \"\\n\" + example['options']['B'] + \"\\n\" + example['options']['C'] + \"\\n\" + example['options']['D'] + \"\\n\\nRespond with the correct choice from the list above verbatim. Do not include any explanation.\"\n",
        "\n",
        "    context = get_relevant_context(example[\"question\"] + \"\\n\\n\" + example['options']['A'] + \"\\n\" + example['options']['B'] + \"\\n\" + example['options']['C'] + \"\\n\" + example['options']['D'])\n",
        "    prompt += \" You may use the following information only if it is helpful: \\n\" + context[0]\n",
        "\n",
        "    prompt += \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "\n",
        "    prediction = predict(prompt)\n",
        "\n",
        "    if prediction not in options:\n",
        "      prompt += prediction + \"<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\nYour response does not exactly match one of the choices from the list. Do not apologise or include any text other than one of the options from the list verbatim without any label. Here are the options again\\n\\n\" + example['options']['A'] + \"\\n\\n\" + example['options']['B'] + \"\\n\\n\" + example['options']['C'] + \"\\n\\n\" + example['options']['D'] + \"\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "      prediction = predict(prompt)\n",
        "\n",
        "    if prediction not in options:\n",
        "      mismatch_cnt += 1\n",
        "\n",
        "    reference = example['options'][example['answer_idx']]\n",
        "\n",
        "    references.append(reference)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "  em = sum([1 if prediction == reference else 0 for prediction, reference in zip(predictions, references)]) / SAMPLE_CNT\n",
        "  mismatch = mismatch_cnt / SAMPLE_CNT\n",
        "\n",
        "  return em, mismatch\n",
        "\n",
        "em, mismatch = eval_medqa()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_eALMg5xbNu",
        "outputId": "9464e368-ec6a-4172-b433-556b48ac2427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exact match: 0.5811965811965812\n",
            "mismatch: 0.01282051282051282\n"
          ]
        }
      ],
      "source": [
        "print(\"exact match:\", em)\n",
        "print(\"mismatch:\", mismatch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZtm8boWzLwO"
      },
      "source": [
        "# pubmedqa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwGsm7rdxbN9"
      },
      "outputs": [],
      "source": [
        "def eval_pubmedqa():\n",
        "  references = []\n",
        "  predictions = []\n",
        "  SAMPLE_CNT = len(pubmedqa)\n",
        "  mismatch_cnt = 0\n",
        "\n",
        "  for i in range(SAMPLE_CNT):\n",
        "    example = pubmedqa[i]\n",
        "\n",
        "    prompt = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\" + example[\"QUESTION\"] + \"\\n\\nRespond with only lower case yes or lowercase no.\"\n",
        "\n",
        "    context = get_relevant_context(example[\"QUESTION\"] + \"\\n\\n\")\n",
        "    prompt += \" You may use the following information only if it is helpful: \\n\" + context[0]\n",
        "\n",
        "    prompt += \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "\n",
        "    prediction = predict(prompt)\n",
        "\n",
        "    if prediction not in [\"yes\", \"no\"]:\n",
        "      prompt += prediction + \"<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\nYour response does not exactly match yes or no. Do not apologise, simply respond with yes or no\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "      prediction = predict(prompt)\n",
        "\n",
        "    if prediction not in [\"yes\", \"no\"]:\n",
        "      mismatch_cnt += 1\n",
        "\n",
        "    reference = example['final_decision']\n",
        "\n",
        "    references.append(reference)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "    em = sum([1 if prediction == reference else 0 for prediction, reference in zip(predictions, references)]) / SAMPLE_CNT\n",
        "\n",
        "    mismatch =  mismatch_cnt / SAMPLE_CNT\n",
        "\n",
        "  return em, mismatch\n",
        "\n",
        "em, mismatch = eval_pubmedqa()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ctlfnzrzp6X"
      },
      "outputs": [],
      "source": [
        "print(\"exact match:\", em)\n",
        "print(\"mismatch:\", mismatch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtIknvuqzy2s"
      },
      "source": [
        "# mmlu anatomy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-LZIehpzycK"
      },
      "outputs": [],
      "source": [
        "def eval_mmlu_anatomy():\n",
        "  references = []\n",
        "  predictions = []\n",
        "  SAMPLE_CNT = len(mmlu_anatomy)\n",
        "  mismatch_cnt = 0\n",
        "\n",
        "  for i in range(SAMPLE_CNT):\n",
        "    example = mmlu_anatomy[i][\"data\"]\n",
        "    options = [example[\"Options\"][\"A\"], example[\"Options\"][\"B\"], example[\"Options\"][\"C\"], example[\"Options\"][\"D\"]]\n",
        "\n",
        "    prompt = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\" + example[\"Question\"] + \"\\n\\n\" + example['Options']['A'] + \"\\n\" + example['Options']['B'] + \"\\n\" + example['Options']['C'] + \"\\n\" + example['Options']['D'] + \"\\n\\nRespond with the correct choice from the list above verbatim. Do not include any explanation.\"\n",
        "\n",
        "    context = get_relevant_context(example[\"Question\"] + \"\\n\\n\" + example['Options']['A'] + \"\\n\" + example['Options']['B'] + \"\\n\" + example['Options']['C'] + \"\\n\" + example['Options']['D'])\n",
        "    prompt += \" You may use the following information only if it is helpful: \\n\" + context[0]\n",
        "\n",
        "    prompt += \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "\n",
        "    prediction = predict(prompt)\n",
        "\n",
        "    if prediction not in options:\n",
        "      prompt += prediction + \"<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\nYour response does not exactly match one of the choices from the list. Do not apologise or include any text other than one of the options from the list verbatim without any label. Here are the options again\\n\\n\" + example['Options']['A'] + \"\\n\\n\" + example['Options']['B'] + \"\\n\\n\" + example['Options']['C'] + \"\\n\\n\" + example['Options']['D'] + \"\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "      prediction = predict(prompt)\n",
        "\n",
        "    if prediction not in options:\n",
        "      mismatch_cnt += 1\n",
        "\n",
        "    reference = example[\"Correct Answer\"]\n",
        "\n",
        "    references.append(reference)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "    em = sum([1 if prediction == reference else 0 for prediction, reference in zip(predictions, references)]) / SAMPLE_CNT\n",
        "    mismatch =  mismatch_cnt / SAMPLE_CNT\n",
        "\n",
        "  return em, mismatch\n",
        "\n",
        "em, mismatch = eval_mmlu_anatomy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFy8TfgCzycL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68721196-fd6f-49c1-f818-f6c81e4d984c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exact match: 0.5777777777777777\n",
            "mismatch 0.05185185185185185\n"
          ]
        }
      ],
      "source": [
        "print(\"exact match:\", em)\n",
        "print(\"mismatch\", mismatch)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
