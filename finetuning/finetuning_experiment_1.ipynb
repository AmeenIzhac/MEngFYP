{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsboHtg6INBb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TrH4FaQ_Sel"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset, ReadInstruction\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = torch.float16,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 128,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S35ZtEvoMxL"
      },
      "source": [
        "# Evaluation Before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFQGgY3HCZ63"
      },
      "outputs": [],
      "source": [
        "dataset_validation = load_dataset(\"openlifescienceai/medmcqa\", split=\"validation\").filter(lambda example: example[\"subject_name\"] == \"Anatomy\")\n",
        "FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EzPfBRu-rq33",
        "outputId": "efffc38d-4074-4ef4-80ae-987e9c79517e"
      },
      "outputs": [],
      "source": [
        "def predict(prompt):\n",
        "    inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "    return tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens = True)[0].strip()\n",
        "\n",
        "def evaluate():\n",
        "  SAMPLE_CNT = len(dataset_validation)\n",
        "  mismatch_cnt = 0\n",
        "  predictions = []\n",
        "  references = []\n",
        "\n",
        "  for i in range(SAMPLE_CNT):\n",
        "    example = dataset_validation[i]\n",
        "    question, option_a, option_b, option_c, option_d = example[\"question\"], example[\"opa\"], example[\"opb\"], example[\"opc\"], example[\"opd\"]\n",
        "    prompt = f'''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{question}\n",
        "\n",
        "{option_a}\n",
        "{option_b}\n",
        "{option_c}\n",
        "{option_d}\n",
        "\n",
        "Respond with the correct choice from the list above verbatim.  Do not include any explanation.<|eot_id|><|start_header_id|>assistant<|end_header_id|>'''\n",
        "\n",
        "    options = [example['opa'], example['opb'], example['opc'], example['opd']]\n",
        "    correct_option = options[example['cop']]\n",
        "    references.append(correct_option)\n",
        "\n",
        "    prediction = predict(prompt)\n",
        "    if prediction not in options:\n",
        "      prompt += prediction + \"<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\nYour response does not exactly match one of the choices from the list. Do not apologise or include any text other than one of the options from the list verbatim without any label. Here are the options again\\n\\n\" + example['opa'] + \"\\n\\n\" + example['opb'] + \"\\n\\n\" + example['opc'] + \"\\n\\n\" + example['opd'] + \"\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "      prediction = predict(prompt)\n",
        "\n",
        "    predictions.append(prediction)\n",
        "\n",
        "    mismatch_cnt += prediction not in options\n",
        "\n",
        "  exact_match = sum([prediction == reference for prediction, reference, in zip(predictions, references)]) / SAMPLE_CNT\n",
        "  mismatch = mismatch_cnt / SAMPLE_CNT\n",
        "\n",
        "  return exact_match, mismatch\n",
        "\n",
        "exact_match, mismatch = evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeI1YqnQu75U",
        "outputId": "a74302ae-4e50-441b-cf4b-466ceebf9b86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "exact_match score: 0.5854700854700855\n",
            "mismatch: 0.01282051282051282\n"
          ]
        }
      ],
      "source": [
        "print(\"exact_match score:\", exact_match)\n",
        "print(\"mismatch:\", mismatch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtJ38Fd6oTeV"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "admpBA8qqHcC"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_training(model)\n",
        "dataset_train = load_dataset(\"openlifescienceai/medmcqa\", split=\"train\").filter(lambda example: example['subject_name'] == 'Anatomy').select(range(0, 3800))\n",
        "dataset_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0LEdsMK4cxB"
      },
      "outputs": [],
      "source": [
        "from datasets import ReadInstruction\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "  texts = []\n",
        "  for question, option_a, option_b, option_c, option_d, answer in zip(examples[\"question\"], examples[\"opa\"], examples[\"opb\"], examples[\"opc\"], examples[\"opd\"], examples[\"exp\"]):\n",
        "\n",
        "    text = f'''<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{question}\n",
        "\n",
        "{option_a}\n",
        "{option_b}\n",
        "{option_c}\n",
        "{option_d}\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{answer}<|eot_id|>'''\n",
        "\n",
        "    texts.append(text)\n",
        "\n",
        "  return { \"text\": texts }\n",
        "\n",
        "dataset_train = dataset_train.map(formatting_prompts_func, batched = True)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset_train,\n",
        "    dataset_text_field = \"text\",\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 32,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 0,\n",
        "        num_train_epochs= 1,\n",
        "        learning_rate = 4e-5,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 42,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kqx7DD3mbqvH"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z6ALD1Xw8Ie"
      },
      "source": [
        "# Evaluation After"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yazYaDy_JD8h"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J_BuaktUXK0Q"
      },
      "outputs": [],
      "source": [
        "exact_match, mismatch = evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IBTDXWF_wyG4",
        "outputId": "a1f1973b-1f9f-4a9d-c9aa-bf7696ef0266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "exact_match: 0.5854700854700855\n",
            "mismatch: 0.01282051282051282\n"
          ]
        }
      ],
      "source": [
        "print(\"exact_match:\", exact_match)\n",
        "print(\"mismatch:\", mismatch)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
