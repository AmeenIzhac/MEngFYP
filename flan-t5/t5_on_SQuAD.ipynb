{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOaQeVhzwODy"
      },
      "outputs": [],
      "source": [
        "! pip install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNHT9U2hsqeu"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9b8n1eew21F"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOwdKcRbEUIT"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXxxu9OIuao8"
      },
      "outputs": [],
      "source": [
        "datasets = load_dataset(\"squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndVAHwCjFpjN"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_tokenize(examples):\n",
        "  input_sequences = []\n",
        "  references = []\n",
        "\n",
        "  for example in examples:\n",
        "    context = example[\"context\"]\n",
        "    question = example[\"question\"]\n",
        "    answer = example[\"answers\"][\"text\"][0]\n",
        "\n",
        "    sequence = context + \" \" + question\n",
        "    input_sequences.append(sequence)\n",
        "    references.append(answer)\n",
        "\n",
        "  input_sequences_and_references = [(sequence, reference) for sequence, reference in zip(input_sequences, references) if is_shorter_than_512(sequence)]\n",
        "  input_sequences, references = zip(*input_sequences_and_references)\n",
        "\n",
        "  tokenized_input_sequences = tokenizer(\n",
        "      input_sequences,\n",
        "      max_length=512,\n",
        "      padding=True,\n",
        "      return_tensors=\"pt\"\n",
        "  )\n",
        "\n",
        "  input_ids, attention_masks = tokenized_input_sequences.input_ids, tokenized_input_sequences.attention_mask\n",
        "\n",
        "  labels = tokenizer(references, padding=\"longest\", return_tensors=\"pt\").input_ids\n",
        "  labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "  return input_ids, attention_masks, labels, references\n",
        "\n",
        "def is_shorter_than_512(sequence):\n",
        "  inputs = tokenizer(sequence, truncation=False)\n",
        "  return len(inputs.input_ids) <= 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02ygwPY0QFmY"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urbMChOeb87d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a665fe-0901-44b9-d40c-3d976751c563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2663: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "input_ids, attention_masks, labels, _ = preprocess_and_tokenize(datasets[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USvhSGWKbJhn"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW, get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "max_source_length = 512\n",
        "max_target_length = 128\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "num_training_steps = len(input_ids) // batch_size\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "\n",
        "for i in range(0, num_training_steps * batch_size, batch_size):\n",
        "  loss = model(input_ids=input_ids[i:i+batch_size].to(device), attention_mask=attention_masks[i:i+batch_size].to(device), labels=labels[i:i+batch_size].to(device)).loss\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  lr_scheduler.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  print(loss.item())\n",
        "  progress_bar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtE4mX0kQApR"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnGxy6ntbT0G"
      },
      "outputs": [],
      "source": [
        "evaluation_input_ids, _, _, references = preprocess_and_tokenize(datasets[\"validation\"])\n",
        "prediction_ids = model.generate(evaluation_input_ids[:1000].to(device))\n",
        "predictions = [tokenizer.decode(prediction_id, skip_special_tokens=True) for prediction_id in prediction_ids]\n",
        "predictions = [\"\" if len(prediction)== 0 else prediction.lower() if prediction[-1] != '.' else prediction[:-1].lower() for prediction in predictions]\n",
        "references = [reference.lower() if reference[-1] != '.' else reference[:-1].lower() for reference in references[:1000]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqsMpkoa2VHz"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHI0448rdxF3"
      },
      "outputs": [],
      "source": [
        "# fixes some wierd bug for below pip install evaluate\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSD-CW_pdi2c"
      },
      "outputs": [],
      "source": [
        "%pip install evaluate\n",
        "%pip install git+https://github.com/google-research/bleurt.git\n",
        "%cd bleurt\n",
        "%pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz3GpYlkb7Xy"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "bleurt = evaluate.load(\"bleurt\", module_type=\"metric\")\n",
        "\n",
        "bleu_references = [[reference] for reference in references.copy()]\n",
        "\n",
        "bleu1_results = bleu.compute(predictions=predictions, references=bleu_references, max_order=1)\n",
        "bleu2_results = bleu.compute(predictions=predictions, references=bleu_references, max_order=2)\n",
        "bleu3_results = bleu.compute(predictions=predictions, references=bleu_references, max_order=3)\n",
        "bleu4_results = bleu.compute(predictions=predictions, references=bleu_references, max_order=4)\n",
        "\n",
        "bleurt_results = bleurt.compute(predictions=predictions, references=references)\n",
        "\n",
        "print(bleu1_results)\n",
        "print(bleu2_results)\n",
        "print(bleu3_results)\n",
        "print(bleu4_results)\n",
        "print(np.mean(bleurt_results[\"scores\"]))\n",
        "\n",
        "for i in range(10):\n",
        "  print(predictions[i], \"|\", references[i], \"|\", bleurt.compute(predictions=predictions[i:i+1], references=references[i:i+1])[\"scores\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6rRj_8c0hE0"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "exact_match_metric = load(\"exact_match\")\n",
        "results = exact_match_metric.compute(predictions=predictions, references=references)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Pbbv2Ky2JK4"
      },
      "outputs": [],
      "source": [
        "def calculate_precision(prediction_tokens, reference_tokens):\n",
        "    prediction_set = set(prediction_tokens)\n",
        "    reference_set = set(reference_tokens)\n",
        "\n",
        "    # Calculate the number of common elements (intersection)\n",
        "    common_tokens = prediction_set.intersection(reference_set)\n",
        "    num_common = len(common_tokens)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    if len(prediction_set) == 0:\n",
        "        precision = 0\n",
        "    else:\n",
        "        precision = num_common / len(prediction_set)\n",
        "\n",
        "    return precision\n",
        "\n",
        "def calculate_recall(prediction_tokens, reference_tokens):\n",
        "    prediction_set = set(prediction_tokens)\n",
        "    reference_set = set(reference_tokens)\n",
        "\n",
        "    # Calculate the number of common elements (intersection)\n",
        "    common_tokens = prediction_set.intersection(reference_set)\n",
        "    num_common = len(common_tokens)\n",
        "\n",
        "    if len(reference_set) == 0:\n",
        "        recall = 0\n",
        "    else:\n",
        "        recall = num_common / len(reference_set)\n",
        "\n",
        "    return recall\n",
        "\n",
        "def calculate_f1_score(prediction_tokens, reference_tokens):\n",
        "    precision = calculate_precision(prediction_tokens, reference_tokens)\n",
        "    recall = calculate_recall(prediction_tokens, reference_tokens)\n",
        "\n",
        "    if (precision + recall) == 0:\n",
        "        f1_score = 0\n",
        "    else:\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    return f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7eOD6dR4Hev"
      },
      "outputs": [],
      "source": [
        "precisions = []\n",
        "recalls = []\n",
        "f1s = []\n",
        "for i in range(100):\n",
        "  precisions.append(calculate_precision(tokenizer(predictions[i])[\"input_ids\"], tokenizer(references[i])[\"input_ids\"]))\n",
        "  recalls.append(calculate_recall(tokenizer(predictions[i])[\"input_ids\"], tokenizer(references[i])[\"input_ids\"]))\n",
        "  f1s.append(calculate_f1_score(tokenizer(predictions[i])[\"input_ids\"], tokenizer(references[i])[\"input_ids\"]))\n",
        "print(np.mean(precisions))\n",
        "print(np.mean(recalls))\n",
        "print(np.mean(f1s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_DFVIiT9t7z"
      },
      "outputs": [],
      "source": [
        "pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lOahWfP8oT5"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "results = rouge.compute(predictions=predictions,references=references)\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}