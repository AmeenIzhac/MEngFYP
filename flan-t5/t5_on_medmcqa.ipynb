{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOaQeVhzwODy"
      },
      "outputs": [],
      "source": [
        "! pip install datasets transformers tiktoken pinecone-client langchain openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNHT9U2hsqeu"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9b8n1eew21F"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOwdKcRbEUIT"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXxxu9OIuao8"
      },
      "outputs": [],
      "source": [
        "datasets = load_dataset(\"openlifescienceai/medmcqa\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOblWZUdjeSn"
      },
      "outputs": [],
      "source": [
        "datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndVAHwCjFpjN"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_tokenize(examples):\n",
        "  input_sequences = []\n",
        "  references = []\n",
        "\n",
        "  for example in examples:\n",
        "    question = \"Instructions: The following is a multiple choice question about medical knowledge. Solve it in a step-by-step fashion, starting by summarizing the available information. Output a single option from the four options as the final answer. Question: \" + example[\"question\"] + \" (A) \" + example[\"opa\"] + \" (B) \" + example[\"opb\"] + \" (C) \" + example[\"opc\"] + \" (D) \" + example[\"opd\"]\n",
        "    answer = [example[\"opa\"], example[\"opb\"], example[\"opc\"], example[\"opd\"]][example[\"cop\"]]\n",
        "\n",
        "    input_sequences.append(question)\n",
        "    references.append(answer)\n",
        "\n",
        "  input_sequences_and_references = [(sequence, reference) for sequence, reference in zip(input_sequences, references) if is_shorter_than_max_length(sequence)]\n",
        "  input_sequences, references = zip(*input_sequences_and_references)\n",
        "\n",
        "  tokenized_input_sequences = tokenizer(\n",
        "      input_sequences,\n",
        "      max_length=512,\n",
        "      padding=True,\n",
        "      return_tensors=\"pt\"\n",
        "  )\n",
        "\n",
        "  input_ids, attention_masks = tokenized_input_sequences.input_ids, tokenized_input_sequences.attention_mask\n",
        "\n",
        "  labels = tokenizer(references, padding=\"longest\", return_tensors=\"pt\").input_ids\n",
        "  labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "  return input_ids, attention_masks, labels, references\n",
        "\n",
        "def is_shorter_than_max_length(sequence):\n",
        "  inputs = tokenizer(sequence, truncation=False)\n",
        "  return len(inputs.input_ids) <= 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02ygwPY0QFmY"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urbMChOeb87d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fea40c64-3fef-4397-c67f-83e7ae6aca9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "input_ids, attention_masks, labels, _ = preprocess_and_tokenize(datasets[\"train\"])\n",
        "\n",
        "val_input_ids, val_attention_masks, val_labels, _ = preprocess_and_tokenize(datasets[\"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USvhSGWKbJhn"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW, get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "max_source_length = 512\n",
        "max_target_length = 128\n",
        "batch_size = 2\n",
        "num_epochs = 1\n",
        "\n",
        "num_training_steps_per_epoch = len(input_ids) // batch_size\n",
        "num_training_steps = num_training_steps_per_epoch * num_epochs\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "\n",
        "val_losses = []\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  val_losses.append(model(input_ids=val_input_ids[i*8:i*8+8].to(device), attention_mask=val_attention_masks[i*8:i*8+8].to(device), labels=val_labels[i*8:i*8+8].to(device)).loss)\n",
        "\n",
        "  for j in range(0, num_training_steps_per_epoch * batch_size, batch_size):\n",
        "    batch_input_ids = input_ids[j:j+batch_size].to(device)\n",
        "    batch_attention_masks = attention_masks[j:j+batch_size].to(device)\n",
        "    batch_labels = labels[j:j+batch_size].to(device)\n",
        "\n",
        "    loss = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels).loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    progress_bar.update(1)\n",
        "\n",
        "val_losses.append(model(input_ids=val_input_ids[3*8:3*8+8].to(device), attention_mask=val_attention_masks[3*8:3*8+8].to(device), labels=val_labels[3*8:3*8+8].to(device)).loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for l in val_losses:\n",
        "  print(l.item())"
      ],
      "metadata": {
        "id": "ohwbrqycNgdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtE4mX0kQApR"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnGxy6ntbT0G"
      },
      "outputs": [],
      "source": [
        "evaluation_input_ids, _, _, references = preprocess_and_tokenize(datasets[\"validation\"])\n",
        "prediction_ids = []\n",
        "for i in range(1000):\n",
        "  prediction_ids.append(model.generate(evaluation_input_ids[i:i + 1].to(device))[0])\n",
        "predictions = [tokenizer.decode(prediction_id, skip_special_tokens=True) for prediction_id in prediction_ids]\n",
        "predictions = [\"\" if len(prediction)== 0 else prediction.lower() if prediction[-1] != '.' else prediction[:-1].lower() for prediction in predictions]\n",
        "references = [reference.lower() if reference[-1] != '.' else reference[:-1].lower() for reference in references[:1000]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Which of the following is not true for myelinated nerve fibers: (A) Impulse through myelinated fibers is slower than non-myelinated fibers (B) Membrane currents are generated at nodes of Ranvier (C) Saltatory conduction of impulses is seen (D) Local anesthesia is effective only when the nerve is not covered by myelin sheath?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "outputs = model.generate(**inputs)\n",
        "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "iE6nB1BZW5iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqsMpkoa2VHz"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHI0448rdxF3"
      },
      "outputs": [],
      "source": [
        "# fixes some wierd bug for below pip install evaluate\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSD-CW_pdi2c"
      },
      "outputs": [],
      "source": [
        "%pip install evaluate\n",
        "%pip install git+https://github.com/google-research/bleurt.git\n",
        "%cd bleurt\n",
        "%pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz3GpYlkb7Xy"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "bleurt = evaluate.load(\"bleurt\", module_type=\"metric\")\n",
        "\n",
        "bleu_references = [[reference] for reference in references.copy()]\n",
        "\n",
        "bleu1_results = bleu.compute(predictions=predictions, references=bleu_references, max_order=1)\n",
        "bleu2_results = bleu.compute(predictions=predictions, references=bleu_references, max_order=2)\n",
        "bleu3_results = bleu.compute(predictions=predictions, references=bleu_references, max_order=3)\n",
        "bleu4_results = bleu.compute(predictions=predictions, references=bleu_references, max_order=4)\n",
        "\n",
        "bleurt_results = bleurt.compute(predictions=predictions, references=references)\n",
        "\n",
        "print(bleu1_results)\n",
        "print(bleu2_results)\n",
        "print(bleu3_results)\n",
        "print(bleu4_results)\n",
        "print(np.mean(bleurt_results[\"scores\"]))\n",
        "\n",
        "for i in range(10):\n",
        "  print(predictions[i], \"|\", references[i], \"|\", bleurt.compute(predictions=predictions[i:i+1], references=references[i:i+1])[\"scores\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiXrblmLb0FF"
      },
      "outputs": [],
      "source": [
        "cnt = 0\n",
        "for i in range(100):\n",
        "  if predictions[i] == \"(a)\" or predictions[i] == \"(b)\" or predictions[i] == \"(c)\" or predictions[i] == \"(d)\":\n",
        "    cnt += 1\n",
        "print(cnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cyhdut72jPN"
      },
      "outputs": [],
      "source": [
        "for i in range(100):\n",
        "  print(predictions[i], \"|\", references[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6rRj_8c0hE0"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "exact_match_metric = load(\"exact_match\")\n",
        "results = exact_match_metric.compute(predictions=predictions, references=references)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Pbbv2Ky2JK4"
      },
      "outputs": [],
      "source": [
        "def calculate_precision(prediction_tokens, reference_tokens):\n",
        "    prediction_set = set(prediction_tokens)\n",
        "    reference_set = set(reference_tokens)\n",
        "\n",
        "    # Calculate the number of common elements (intersection)\n",
        "    common_tokens = prediction_set.intersection(reference_set)\n",
        "    num_common = len(common_tokens)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    if len(prediction_set) == 0:\n",
        "        precision = 0\n",
        "    else:\n",
        "        precision = num_common / len(prediction_set)\n",
        "\n",
        "    return precision\n",
        "\n",
        "def calculate_recall(prediction_tokens, reference_tokens):\n",
        "    prediction_set = set(prediction_tokens)\n",
        "    reference_set = set(reference_tokens)\n",
        "\n",
        "    # Calculate the number of common elements (intersection)\n",
        "    common_tokens = prediction_set.intersection(reference_set)\n",
        "    num_common = len(common_tokens)\n",
        "\n",
        "    if len(reference_set) == 0:\n",
        "        recall = 0\n",
        "    else:\n",
        "        recall = num_common / len(reference_set)\n",
        "\n",
        "    return recall\n",
        "\n",
        "def calculate_f1_score(prediction_tokens, reference_tokens):\n",
        "    precision = calculate_precision(prediction_tokens, reference_tokens)\n",
        "    recall = calculate_recall(prediction_tokens, reference_tokens)\n",
        "\n",
        "    if (precision + recall) == 0:\n",
        "        f1_score = 0\n",
        "    else:\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    return f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7eOD6dR4Hev"
      },
      "outputs": [],
      "source": [
        "precisions = []\n",
        "recalls = []\n",
        "f1s = []\n",
        "for i in range(100):\n",
        "  precisions.append(calculate_precision(tokenizer(predictions[i])[\"input_ids\"], tokenizer(references[i])[\"input_ids\"]))\n",
        "  recalls.append(calculate_recall(tokenizer(predictions[i])[\"input_ids\"], tokenizer(references[i])[\"input_ids\"]))\n",
        "  f1s.append(calculate_f1_score(tokenizer(predictions[i])[\"input_ids\"], tokenizer(references[i])[\"input_ids\"]))\n",
        "print(np.mean(precisions))\n",
        "print(np.mean(recalls))\n",
        "print(np.mean(f1s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_DFVIiT9t7z"
      },
      "outputs": [],
      "source": [
        "pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lOahWfP8oT5"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "results = rouge.compute(predictions=predictions,references=references)\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}