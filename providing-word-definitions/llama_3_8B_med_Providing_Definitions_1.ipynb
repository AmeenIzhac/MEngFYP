{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwuW84ehxkT5"
      },
      "source": [
        "# imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsboHtg6INBb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install \"xformers<0.0.26\" trl peft accelerate bitsandbytes pinecone-client sentence-transformers langchain-openai openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJExmmYaAdee"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TrH4FaQ_Sel"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = torch.float16,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 256,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 512,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOTDVBOMEQYQ"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key='INSERT_PINECONE_API_KEY_HERE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8SBuaNCFxH1"
      },
      "outputs": [],
      "source": [
        "index = pc.Index(\"medmcqa-train-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys16bnn7JdEX"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=\"INSERT_OPEN_API_KEY_HERE\")\n",
        "\n",
        "def get_relevant_context(text, k=1):\n",
        "  embeds = embed_model.embed_documents([text])\n",
        "  results_1 = index.query(\n",
        "      vector=embeds[0],\n",
        "      top_k=k,\n",
        "      include_metadata=True\n",
        "  )\n",
        "\n",
        "  results = []\n",
        "\n",
        "  for result in results_1[\"matches\"]:\n",
        "    results.append(result[\"metadata\"][\"text\"])\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MNB8HXQxfYw"
      },
      "source": [
        "# load datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbZadzvn-Hb7"
      },
      "outputs": [],
      "source": [
        "medmcqa = load_dataset(\"openlifescienceai/medmcqa\", split=\"validation\")\n",
        "medmcqa_anatomy = medmcqa.filter(lambda example: example[\"subject_name\"] == \"Anatomy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOcUQyodCGWH"
      },
      "outputs": [],
      "source": [
        "print(medmcqa_anatomy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFQGgY3HCZ63"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o74N2rVoCkRL"
      },
      "outputs": [],
      "source": [
        "def predict(prompt):\n",
        "    inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "    return tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens = True)[0].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH6LIAfCsSnG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('word_defs.json') as file:\n",
        "  word_defs = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHQLbWfqymDx"
      },
      "source": [
        "# medmcqa anatomy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EzPfBRu-rq33"
      },
      "outputs": [],
      "source": [
        "def eval_medmcqa_anatomy():\n",
        "  references = []\n",
        "  predictions = []\n",
        "  SAMPLE_CNT = len(medmcqa_anatomy)\n",
        "  mismatch_cnt = 0\n",
        "\n",
        "  for i in range(SAMPLE_CNT):\n",
        "    example = medmcqa_anatomy[i]\n",
        "    options = [example['opa'], example['opb'], example['opc'], example['opd']]\n",
        "\n",
        "    prompt = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\" + example[\"question\"] + \"\\n\\n\" + example['opa'] + \"\\n\" + example['opb'] + \"\\n\" + example['opc'] + \"\\n\" + example['opd'] + \"\\n\\nRespond with the correct choice from the list above verbatim.  Do not include any explanation.\"\n",
        "\n",
        "    if all(option in word_defs.keys() for option in options):\n",
        "      prompt += \" You may use the following definitions only if they are helpful: \\n\"\n",
        "      for option in options:\n",
        "        if option in word_defs.keys():\n",
        "          prompt += f'''{option}: {word_defs[option]}\\n'''\n",
        "\n",
        "    prompt += \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    prediction = predict(prompt)\n",
        "\n",
        "    if prediction not in options:\n",
        "      prompt += prediction + \"<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\nYour response does not exactly match one of the choices from the list. Do not apologise or include any text other than one of the options from the list verbatim without any label. Here are the options again\\n\\n\" + example['opa'] + \"\\n\\n\" + example['opb'] + \"\\n\\n\" + example['opc'] + \"\\n\\n\" + example['opd'] + \"\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "      prediction = predict(prompt)\n",
        "\n",
        "    reference = options[example['cop']]\n",
        "\n",
        "    if prediction not in options:\n",
        "      mismatch_cnt += 1\n",
        "\n",
        "    references.append(reference)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "  em = sum([1 if prediction == reference else 0 for prediction, reference in zip(predictions, references)]) / SAMPLE_CNT\n",
        "  mismatch = mismatch_cnt / SAMPLE_CNT\n",
        "  return em, mismatch\n",
        "\n",
        "em, mismatch = eval_medmcqa_anatomy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeI1YqnQu75U",
        "outputId": "0ebc208a-03f3-44ca-a667-fcbf8119f24a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "exact match: 0.5854700854700855\n",
            "mismatch: 0.017094017094017096\n"
          ]
        }
      ],
      "source": [
        "print(\"exact match:\", em)\n",
        "print(\"mismatch:\", mismatch)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
